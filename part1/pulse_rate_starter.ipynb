{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pulse Rate Algorithm\n",
    "\n",
    "### Contents\n",
    "Fill out this notebook as part of your final project submission.\n",
    "\n",
    "**You will have to complete both the Code and Project Write-up sections.**\n",
    "- The [Code](#Code) is where you will write a **pulse rate algorithm** and already includes the starter code.\n",
    "   - Imports - These are the imports needed for Part 1 of the final project. \n",
    "     - [glob](https://docs.python.org/3/library/glob.html)\n",
    "     - [numpy](https://numpy.org/)\n",
    "     - [scipy](https://www.scipy.org/)\n",
    "- The [Project Write-up](#Project-Write-up) to describe why you wrote the algorithm for the specific case.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "You will be using the **Troika**[1] dataset to build your algorithm. Find the dataset under `datasets/troika/training_data`. The `README` in that folder will tell you how to interpret the data. The starter code contains a function to help load these files.\n",
    "\n",
    "1. Zhilin Zhang, Zhouyue Pi, Benyuan Liu, ‘‘TROIKA: A General Framework for Heart Rate Monitoring Using Wrist-Type Photoplethysmographic Signals During Intensive Physical Exercise,’’IEEE Trans. on Biomedical Engineering, vol. 62, no. 2, pp. 522-531, February 2015. Link\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.5486103362072843"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "\n",
    "def LoadTroikaDataset():\n",
    "    \"\"\"\n",
    "    Retrieve the .mat filenames for the troika dataset.\n",
    "\n",
    "    Review the README in ./datasets/troika/ to understand the organization of the .mat files.\n",
    "\n",
    "    Returns:\n",
    "        data_fls: Names of the .mat files that contain signal data\n",
    "        ref_fls: Names of the .mat files that contain reference data\n",
    "        <data_fls> and <ref_fls> are ordered correspondingly, so that ref_fls[5] is the \n",
    "            reference data for data_fls[5], etc...\n",
    "    \"\"\"\n",
    "    data_dir = \"./datasets/troika/training_data\"\n",
    "    data_fls = sorted(glob.glob(data_dir + \"/DATA_*.mat\"))\n",
    "    ref_fls = sorted(glob.glob(data_dir + \"/REF_*.mat\"))\n",
    "    return data_fls, ref_fls\n",
    "\n",
    "def LoadTroikaDataFile(data_fl):\n",
    "    \"\"\"\n",
    "    Loads and extracts signals from a troika data file.\n",
    "\n",
    "    Usage:\n",
    "        data_fls, ref_fls = LoadTroikaDataset()\n",
    "        ppg, accx, accy, accz = LoadTroikaDataFile(data_fls[0])\n",
    "\n",
    "    Args:\n",
    "        data_fl: (str) filepath to a troika .mat file.\n",
    "\n",
    "    Returns:\n",
    "        numpy arrays for ppg, accx, accy, accz signals.\n",
    "    \"\"\"\n",
    "    data = sp.io.loadmat(data_fl)['sig']\n",
    "    return data[2:]\n",
    "\n",
    "def PreprocessTestData(data_fl, ref_fl):   \n",
    "    fs=125\n",
    "    win_len = 8\n",
    "    win_shift = 2\n",
    "    \n",
    "    sig = LoadTroikaDataFile(data_fl)\n",
    "    ref = scipy.io.loadmat(ref_fl)[\"BPM0\"]\n",
    "    ref = np.array([x[0] for x in ref])\n",
    "    subject_name = os.path.basename(data_fl).split('.')[0]        \n",
    "    start_indxs, end_indxs = get_indxs(sig.shape[1], len(ref), fs, win_len,win_shift)\n",
    "    targets, features, sigs, subs = [], [], [], []\n",
    "    for i, s in enumerate(start_indxs):\n",
    "        start_i =  start_indxs[i]\n",
    "        end_i = end_indxs[i]\n",
    "\n",
    "        ppg = sig[0, start_i:end_i]            \n",
    "        accx = sig[1, start_i:end_i]\n",
    "        accy = sig[2, start_i:end_i]\n",
    "        accz = sig[3, start_i:end_i]\n",
    "\n",
    "        ppg = BandpassFilter(ppg)\n",
    "        accx = BandpassFilter(accx)\n",
    "        accy = BandpassFilter(accy)\n",
    "        accz = BandpassFilter(accz)\n",
    "\n",
    "        feature, ppg, accx, accy, accz = Featurize(ppg, accx, accy, accz)\n",
    "\n",
    "        sigs.append([ppg, accx, accy, accz])\n",
    "        targets.append(ref[i])\n",
    "        features.append(feature)\n",
    "        subs.append(subject_name)\n",
    "        \n",
    "    return (np.array(targets), np.array(features), sigs, subs)\n",
    "\n",
    "def AggregateErrorMetric(pr_errors, confidence_est):\n",
    "    \"\"\"\n",
    "    Computes an aggregate error metric based on confidence estimates.\n",
    "\n",
    "    Computes the MAE at 90% availability. \n",
    "\n",
    "    Args:\n",
    "        pr_errors: a numpy array of errors between pulse rate estimates and corresponding \n",
    "            reference heart rates.\n",
    "        confidence_est: a numpy array of confidence estimates for each pulse rate\n",
    "            error.\n",
    "\n",
    "    Returns:\n",
    "        the MAE at 90% availability\n",
    "    \"\"\"\n",
    "    # Higher confidence means a better estimate. The best 90% of the estimates\n",
    "    #    are above the 10th percentile confidence.\n",
    "    percentile90_confidence = np.percentile(confidence_est, 10)\n",
    "\n",
    "    # Find the errors of the best pulse rate estimates\n",
    "    best_estimates = pr_errors[confidence_est >= percentile90_confidence]\n",
    "\n",
    "    # Return the mean absolute error\n",
    "    return np.mean(np.abs(best_estimates))\n",
    "\n",
    "def Featurize(ppg, accx, accy, accz):\n",
    "    \"\"\" Create features \"\"\"\n",
    "\n",
    "    fs = 125\n",
    "    n = len(ppg) * 4\n",
    "    freqs = np.fft.rfftfreq(n, 1/fs)\n",
    "    fft = np.abs(np.fft.rfft(ppg,n))\n",
    "    fft[freqs <= 40/60.0] = 0.0\n",
    "    fft[freqs >= 240/60.0] = 0.0\n",
    "    \n",
    "    acc_mag = np.sqrt(accx**2 + accy**2 + accz**2)\n",
    "    acc_fft = np.abs(np.fft.rfft(acc_mag, n))\n",
    "    acc_fft[freqs <= 40/60.0] = 0.0\n",
    "    acc_fft[freqs >= 240/60.0] = 0.0\n",
    "    \n",
    "    ppg_feature = freqs[np.argmax(fft)]\n",
    "    acc_feature = freqs[np.argmax(acc_fft)]\n",
    "    \n",
    "    return (np.array([ppg_feature, acc_feature]), ppg, accx, accy, accz)\n",
    "\n",
    "def Evaluate():\n",
    "    \"\"\"\n",
    "    Top-level function evaluation function.\n",
    "\n",
    "    Runs the pulse rate algorithm on the Troika dataset and returns an aggregate error metric.\n",
    "\n",
    "    Returns:\n",
    "        Pulse rate error on the Troika dataset. See AggregateErrorMetric.\n",
    "    \"\"\"\n",
    "    global reg\n",
    "    reg = TrainRegressor()\n",
    "    \n",
    "    data_fls, ref_fls = LoadTroikaDataset()\n",
    "    errs, confs = [], []\n",
    "    for data_fl, ref_fl in zip(data_fls, ref_fls):\n",
    "        errors, confidence = RunPulseRateAlgorithm(data_fl, ref_fl)\n",
    "        errs.append(errors)\n",
    "        confs.append(confidence)\n",
    "        \n",
    "    errs = np.hstack(errs)\n",
    "    confs = np.hstack(confs)\n",
    "    return AggregateErrorMetric(errs, confs)\n",
    "\n",
    "def BandpassFilter(signal, fs = 125):   \n",
    "    b, a = scipy.signal.butter(3, (40/60.0, 240/60.0), btype='bandpass', fs=fs)\n",
    "    return scipy.signal.filtfilt(b, a, signal)\n",
    "\n",
    "def get_indxs(sig_len, ref_len, fs=125, win_len_s=10, win_shift_s=2):\n",
    "    \"\"\"\n",
    "    Find start and end index to iterate over a set of signals\n",
    "    \"\"\"\n",
    "    if ref_len < sig_len:\n",
    "        n = ref_len\n",
    "    else:\n",
    "        n = sig_len\n",
    "    \n",
    "    start_indxs = (np.cumsum(np.ones(n) * fs * win_shift_s) - fs * win_shift_s).astype(int)\n",
    "    end_indxs = start_indxs + win_len_s * fs\n",
    "    return (start_indxs, end_indxs)\n",
    "\n",
    "def TrainRegressor():\n",
    "    \n",
    "    fs=125\n",
    "    win_len = 8\n",
    "    win_shift = 2\n",
    "    \n",
    "    data_fls, ref_fls = LoadTroikaDataset()\n",
    "    targets, features, sigs, subs = [], [], [], []\n",
    "    for data_fl, ref_fl in (zip(data_fls, ref_fls)):\n",
    "        \n",
    "        sig = LoadTroikaDataFile(data_fl)\n",
    "        ref = scipy.io.loadmat(ref_fl)[\"BPM0\"]\n",
    "        ref = np.array([x[0] for x in ref])\n",
    "        subject_name = os.path.basename(data_fl).split('.')[0]        \n",
    "        start_indxs, end_indxs = get_indxs(sig.shape[1], len(ref), fs, win_len,win_shift)\n",
    "        for i, s in enumerate(start_indxs):\n",
    "            start_i =  start_indxs[i]\n",
    "            end_i = end_indxs[i]\n",
    "\n",
    "            ppg = sig[0, start_i:end_i]            \n",
    "            accx = sig[1, start_i:end_i]\n",
    "            accy = sig[2, start_i:end_i]\n",
    "            accz = sig[3, start_i:end_i]\n",
    "\n",
    "            ppg = BandpassFilter(ppg)\n",
    "            accx = BandpassFilter(accx)\n",
    "            accy = BandpassFilter(accy)\n",
    "            accz = BandpassFilter(accz)\n",
    "\n",
    "            feature, ppg, accx, accy, accz = Featurize(ppg, accx, accy, accz)\n",
    "\n",
    "            sigs.append([ppg, accx, accy, accz])\n",
    "            targets.append(ref[i])\n",
    "            features.append(feature)\n",
    "            subs.append(subject_name)\n",
    "            \n",
    "    targets = np.array(targets)\n",
    "    features = np.array(features)\n",
    "    \n",
    "    regression = RandomForestRegressor(n_estimators=400,max_depth=20)\n",
    "    lf = KFold(n_splits=5)\n",
    "    splits = lf.split(features,targets,subs)\n",
    "\n",
    "    for i, (train_idx, test_idx) in enumerate(splits):\n",
    "        X_train, y_train = features[train_idx], targets[train_idx]\n",
    "        X_test, y_test = features[test_idx], targets[test_idx]\n",
    "        regression.fit(X_train, y_train)\n",
    "    \n",
    "    return regression\n",
    "\n",
    "def RunPulseRateAlgorithm(data_fl, ref_fl):\n",
    "    fs = 125\n",
    "    win_len = 8\n",
    "    win_shift = 2    \n",
    "       \n",
    "    targets, features, sigs, subs = PreprocessTestData(data_fl, ref_fl)\n",
    "\n",
    "    error, confidence = [], []\n",
    "    for i,feature in enumerate(features):\n",
    "        est = reg.predict(np.reshape(feature, (1, -1)))[0]\n",
    "        ppg, accx, accy, accz = sigs[i]        \n",
    "        \n",
    "        n = len(ppg) * 4\n",
    "        freqs = np.fft.rfftfreq(n, 1/fs)\n",
    "        fft = np.abs(np.fft.rfft(ppg,n))\n",
    "        fft[freqs <= 40/60.0] = 0.0\n",
    "        fft[freqs >= 240/60.0] = 0.0\n",
    "    \n",
    "        est_fs = est / 55.0\n",
    "        fs_win = 30  / 60.0\n",
    "        fs_win_e = (freqs >= est_fs - fs_win) & (freqs <= est_fs +fs_win)\n",
    "        conf = np.sum(fft[fs_win_e])/np.sum(fft)\n",
    "        \n",
    "        error.append(np.abs((est-targets[i])))\n",
    "        confidence.append(conf)\n",
    "    return np.array(error), np.array(confidence)\n",
    "Evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Project Write-up\n",
    "\n",
    "Answer the following prompts to demonstrate understanding of the algorithm you wrote for this specific context.\n",
    "\n",
    "> - **Code Description** \n",
    ">   - Run the code cell below to check the performance of the algorithm on the training data.\n",
    ">   - Testing data are available in the directory datasets/troika/testing_data if you'd like to see the performance of the model on new data.\n",
    "> - **Data Description** \n",
    "    - ECG signals have one channel.\n",
    "    - PPG signals have two channels. we take the second channel as it poses the more challenging problem and suggested. Here in the problem i have taken mean of both channels as it gives more accurate results.\n",
    "    - Accelerometers have three channels, each corresponding to a space axis x, y, and z. I use the magnitude of these three channels as distance calculation.   \n",
    "> - **Algorithhm Description** \n",
    ">   - RandomForestRegression on featurise data.\n",
    ">   - the specific aspects of the physiology that it takes advantage of : PPG signals can be used for measuring heart rate. Capillaries in the wrist fill with blood when the ventricles contract, when the blood passes light emitted by the PPG sensor is absorbed by red blood cells in these capillaries and the photodetector will see the cut in reflected light. Change in light measures and this oscillating waveform is the pulse rate.\n",
    ">   - a describtion of the algorithm outputs :\n",
    ">      - Outputs: the estimated frequency (in BPM) and the confidence score of that prediction.\n",
    ">   - caveats on algorithm outputs : The confidence rate is only calculated based on the magnitude of a small area that contains the estimated spectral frequency relative to the sum magnitude of the entire spectrum.\n",
    ">   - common failure modes : When the PPG picks a higher frequency signal that is not from the heart rate. This is possible due to hand movements, arm movement, alivation. To overcome with this, the accelerations measurmnet use in the algorithm.\n",
    "> - **Algorithm Performance** \n",
    ">   - The performance was calculated by calculating the mean absolute error between the HR estimation and the reference HR from the ECG sensors at 90% availability. Put another way, 90% of the best estimates according to the algorithm's confidence scores.\n",
    ">   - The error of the model in the testing data is 0.60\n",
    "\n",
    "The algorithm requires its parameters to have the exact values that are currently used as reference to the desired low error score. To improve performance, a machine learning approach can be usefull, with also more test subjects performing various kinds of different activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Next Steps\n",
    "You will now go to **Test Your Algorithm** to apply a unit test to confirm that your algorithm met the success criteria. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
